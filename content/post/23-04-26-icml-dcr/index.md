---
title: "Interpretable Neural-Symbolic Concept Reasoning" is accepted at ICML 2023!
date: 2023-04-20
image:
  focal_point: 'top'
---

The paper "Interpretable Neural-Symbolic Concept Reasoning" has been accepted at ICML 2023!

**Abstract**
Deep learning methods are highly accurate, yet their opaque decision process prevents 
them from earning full human trust. Concept-based models aim to address this issue by 
learning tasks based on a set of human-understandable concepts. However, state-of-the-art 
concept-based models rely on high-dimensional concept embedding representations which 
lack a clear semantic meaning, thus questioning the interpretability of their decision 
process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first 
interpretable concept-based model that builds upon concept embeddings. 
In DCR, neural networks do not make task predictions directly, but they build syntactic 
rule structures using concept embeddings. DCR then executes these rules on meaningful 
concept truth degrees to provide a final interpretable and semantically-consistent 
prediction in a differentiable manner. Our experiments show that DCR: 
(i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based models on challenging benchmarks 
(ii) discovers meaningful logic rules matching known ground truths even in the 
absence of concept supervision during training, and 
(iii), facilitates the generation of counterfactual examples providing the learnt rules as guidance.

**Link**
[ArXiv](https://arxiv.org/abs/2304.14068)
